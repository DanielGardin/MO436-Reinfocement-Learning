{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo Control with Q-value function approximation\n",
    "# Policy evaluation: Q(s, a) <- Q(s, a) + (G - Q(s, a)) / N(s, a)\n",
    "# Policy improvement: epsilon-greedy exploration\n",
    "# Q-value function approximation: Two-layer perception (input layer and output layer only)\n",
    "\n",
    "\n",
    "from pacman.actions import Actions\n",
    "from pacman.agents import Agent\n",
    "from pacman.env import PacmanEnv\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m env \u001b[38;5;241m=\u001b[39m PacmanEnv\u001b[38;5;241m.\u001b[39mcontourDanger(\u001b[38;5;241m7\u001b[39m, ghost_name\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFollowGhost\u001b[39m\u001b[38;5;124m\"\u001b[39m,render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, config\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTIME_PENALTY\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m     61\u001b[0m mc \u001b[38;5;241m=\u001b[39m MonteCarlo(env \u001b[38;5;241m=\u001b[39m env, gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.99\u001b[39m, eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m---> 62\u001b[0m \u001b[43mmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m, in \u001b[0;36mMonteCarlo.train\u001b[0;34m(self, max_episodes, tol)\u001b[0m\n\u001b[1;32m     26\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m     27\u001b[0m Ntable[state][action] \u001b[38;5;241m=\u001b[39m Ntable[state][action] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 29\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m G\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m*\u001b[39mG) \u001b[38;5;241m+\u001b[39m reward\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQtable[state][action] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (G \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQtable[state][action])\u001b[38;5;241m/\u001b[39mNtable[state][action]\n",
      "File \u001b[0;32m~/Documents/MO436-Reinfocement-Learning/pacman/env.py:497\u001b[0m, in \u001b[0;36mPacmanEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    495\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m \u001b[43mghost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_action\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mghost_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_collision(ghost):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ghost\u001b[38;5;241m.\u001b[39mis_scared():\n",
      "File \u001b[0;32m~/Documents/MO436-Reinfocement-Learning/pacman/agents.py:61\u001b[0m, in \u001b[0;36mGhost.apply_action\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m Actions\u001b[38;5;241m.\u001b[39mNOOP:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_legal_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m legal_actions:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIllegal ghost action \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/MO436-Reinfocement-Learning/pacman/env.py:297\u001b[0m, in \u001b[0;36mPacmanEnv.get_legal_actions\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m    294\u001b[0m     new_pos \u001b[38;5;241m=\u001b[39m discrete((x_int \u001b[38;5;241m+\u001b[39m dx, y_int \u001b[38;5;241m+\u001b[39m dy))\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhaswall(new_pos):\n\u001b[0;32m--> 297\u001b[0m         \u001b[43mlegal_actions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legal_actions\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#logger = getLogger(__name__)\n",
    "\n",
    "\n",
    "class MonteCarlo(Agent):\n",
    "    def __init__(self, gamma, env, eps):\n",
    "        #self.alpha  = alpha\n",
    "        self.gamma  = gamma\n",
    "        self.env    = env\n",
    "        self.eps    = eps\n",
    "        self.Qtable = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "    \n",
    "    def train(self, max_episodes=None, tol=1e-7):\n",
    "        curr_episode = 0\n",
    "        Ntable  = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "\n",
    "        if max_episodes is None:\n",
    "            max_episodes = np.inf\n",
    "\n",
    "\n",
    "        while curr_episode < max_episodes:\n",
    "            #errors = []\n",
    "            state, done = self.env.reset()\n",
    "            G=0\n",
    "\n",
    "            while not done:\n",
    "                action = self.act(state)\n",
    "                Ntable[state][action] = Ntable[state][action] + 1\n",
    "               \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                G= (self.gamma*G) + reward\n",
    "\n",
    "                self.Qtable[state][action] += (G - self.Qtable[state][action])/Ntable[state][action]\n",
    "\n",
    "                #next_action = self.act(next_state)\n",
    "\n",
    "                #td_error = reward + self.gamma * self.Qtable[next_state][next_action] - self.Qtable[state][action]\n",
    "                #errors.append(abs(td_error))\n",
    "                #self.update_tables(Ntable, td_error)\n",
    "\n",
    "                state  = next_state\n",
    "                #action = next_action\n",
    "\n",
    "            self.eps = 0.99*self.eps\n",
    "\n",
    "\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() > self.eps:\n",
    "            action = max(self.Qtable[state], key=self.Qtable[state].get)\n",
    "\n",
    "        else:\n",
    "            action = Actions.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def eval(self):\n",
    "        self.eps = 0\n",
    "\n",
    "env = PacmanEnv.contourDanger(7, ghost_name= \"FollowGhost\",render_mode=None, config={\"TIME_PENALTY\" : 1})\n",
    "mc = MonteCarlo(env = env, gamma = 0.99, eps = 1) \n",
    "mc.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa.eval()\n",
    "env.set_render('ansi')\n",
    "env.run_policy(mc, 0, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
