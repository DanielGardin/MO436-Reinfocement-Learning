{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.1)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from pacman.actions import Actions\n",
    "from pacman.env import PacmanEnv\n",
    "from pacman.agents import Agent\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning (MO436)\n",
    "\n",
    "Autores:\n",
    "\n",
    "**Aline Cavalca Caravlho Soares de Azevedo** (RA: )\n",
    "\n",
    "**Beatriz Cardoso Nascimento**               (RA: )\n",
    "\n",
    "**Daniel Gardin Gratti**                     (RA: 214729)\n",
    "\n",
    "**Guilhermo de Luiggi Mocelim de Oliveira** (RA: )\n",
    "\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Reinforcement Learning é um paradigma de Inteligência Artificial baseada em controle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PacmanEnv.contourDanger(4, ghost_names= \"RobustGhost\", state_space=\"features\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episodes(history, metric:str, smooth=11, alpha=0.4):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    episodes = np.array(history[metric])\n",
    "\n",
    "    n_episode = np.array(range(len(episodes)))\n",
    "\n",
    "    plt.plot(episodes, alpha=alpha)\n",
    "\n",
    "    padded_episodes = np.pad(episodes, (smooth//2,smooth//2), 'edge')\n",
    "    smooth_episodes = np.convolve(padded_episodes, np.ones(smooth)/smooth, mode='valid')\n",
    "\n",
    "    ax.plot(smooth_episodes, linewidth=2)\n",
    "\n",
    "    ax.set_title(f\"{metric.capitalize()} during training\")\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel(\"episode\")\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pacman.distributions import UniformDistribution\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def get_distribution(self, state):\n",
    "        return UniformDistribution(Actions.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyQAgent(Agent):\n",
    "    \"\"\"\n",
    "    Agent with internal Q table and epsilon-greedy policy. Can learn from a control algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.Qtable = defaultdict(lambda : {action: 0. for action in Actions.actions})\n",
    "    \n",
    "    \n",
    "    def act(self, state, epsilon=0.) -> str:\n",
    "        if np.random.rand() > epsilon:\n",
    "            action, _ = self.max_Q(state)\n",
    "\n",
    "        else:\n",
    "            action = Actions.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def Q(self, state, action):\n",
    "        return self.Qtable[state][action]\n",
    "    \n",
    "\n",
    "    def max_Q(self, state):\n",
    "        action_values = self.Qtable[state]\n",
    "\n",
    "        action = max(action_values, key=lambda k : action_values[k])\n",
    "\n",
    "        return action, action_values[action]\n",
    "    \n",
    "\n",
    "    def update_Q(self, state, action, alpha, error):\n",
    "        self.Qtable[state][action] += alpha * error\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset agent's Q table.\n",
    "        \"\"\"\n",
    "        self.Qtable.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseController:\n",
    "    def __init__(self, env):\n",
    "        self.env    = env\n",
    "\n",
    "    def train(self, policy, max_episodes):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:55: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:59: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:55: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "<>:59: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_10232\\3859974118.py:55: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  error = sum([history][\"episode error\"])\n",
      "C:\\Users\\guilh\\AppData\\Local\\Temp\\ipykernel_10232\\3859974118.py:59: SyntaxWarning: list indices must be integers or slices, not str; perhaps you missed a comma?\n",
      "  error = sum([history][\"episode error\"][-50 : ])\n"
     ]
    }
   ],
   "source": [
    "class MonteCarloControl(BaseController):\n",
    "    def __init__(self, env, gamma, alpha=None, N0=1):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.N0    = N0\n",
    "    \n",
    "\n",
    "    def train(self, policy, max_episodes=1000):\n",
    "        history = {\n",
    "            \"cumulative reward\"  : [],\n",
    "            \"episode error\"      : [],\n",
    "            \"episode length\"     : [],\n",
    "            \"episode win\"        : []\n",
    "        }\n",
    "\n",
    "        Ntable  = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "\n",
    "        for _ in tqdm(range(max_episodes)):\n",
    "\n",
    "            state, done = self.env.reset(random_init=True)\n",
    "            cumulative_reward = 0\n",
    "            cumulative_error  = 0\n",
    "            n_steps           = 0\n",
    "\n",
    "            while not done:\n",
    "                epsilon = self.N0 / (self.N0 + sum(Ntable[state].values()))\n",
    "\n",
    "                action = policy.act(state, epsilon)\n",
    "\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                Ntable[state][action] += 1\n",
    "                n_steps += 1\n",
    "               \n",
    "                cumulative_reward = (self.gamma * cumulative_reward) + reward\n",
    "\n",
    "                alpha = 1/Ntable[state][action] if self.alpha is None else self.alpha\n",
    "                error = (cumulative_reward - policy.Q(state, action))\n",
    "                policy.update_Q(state, action, alpha, error)\n",
    "\n",
    "                cumulative_error += abs(error)\n",
    "\n",
    "                state  = next_state\n",
    "            \n",
    "            mean_error = cumulative_error / self.env.current_step\n",
    "\n",
    "            history[\"cumulative reward\"].append(cumulative_reward)\n",
    "            history[\"episode error\"].append(mean_error)\n",
    "            history[\"episode length\"].append(n_steps)\n",
    "            history[\"episode win\"].append(env.is_win())\n",
    "\n",
    "            if history[\"episode error\"] < 50:\n",
    "                error = sum([history][\"episode error\"])\n",
    "                error = error / len(history[\"episode error\"])\n",
    "\n",
    "            else:\n",
    "                error = sum([history][\"episode error\"][-50 : ])\n",
    "                error = error / 50\n",
    "\n",
    "            if error < 0.1:\n",
    "                break\n",
    "\n",
    "\n",
    "        \n",
    "        return policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edde9ed9ecd4f0abd2f907a14ea0f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m50\u001b[39m):\n\u001b[0;32m      6\u001b[0m     monte_carlo\u001b[38;5;241m.\u001b[39mN0 \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m----> 7\u001b[0m     _, mc_history \u001b[38;5;241m=\u001b[39m \u001b[43mmonte_carlo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmc_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#print(scores)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mMonteCarloControl.train\u001b[1;34m(self, policy, max_episodes)\u001b[0m\n\u001b[0;32m     25\u001b[0m n_steps           \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 28\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN0 \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN0 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[43mNtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[0;32m     30\u001b[0m     action \u001b[38;5;241m=\u001b[39m policy\u001b[38;5;241m.\u001b[39mact(state, epsilon)\n\u001b[0;32m     32\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "monte_carlo = MonteCarloControl(env.set_render(None), 0.99)\n",
    "mc_policy = EpsilonGreedyQAgent()\n",
    "scores = np.zeros([49,100])\n",
    "\n",
    "for i in range (1,50):\n",
    "    monte_carlo.N0 = i\n",
    "    _, mc_history = monte_carlo.train(mc_policy, 10000)\n",
    "\n",
    "    #print(scores)\n",
    "\n",
    "    for j in range(100):\n",
    "        env.run_policy(mc_policy, 0, 0)\n",
    "        scores[i-1][j] =  env.get_score()\n",
    "\n",
    "    mc_policy.reset()\n",
    "\n",
    "media = np.mean(scores, 1)\n",
    "print(media)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n0 = []\n",
    "for i in range (0,49):\n",
    "    if media[i] >= 43:\n",
    "        best_n0.append(i)\n",
    "\n",
    "best_n0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo = MonteCarloControl(\n",
    "    env    = env.set_render(None),\n",
    "    gamma  = 0.99,\n",
    "    N0     = 19\n",
    ")\n",
    "mc_policy, mc_history = monte_carlo.train(EpsilonGreedyQAgent(), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_render('human')\n",
    "env.run_policy(mc_policy, 0, .5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes(mc_history, \"cumulative reward\", smooth=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning(BaseController):\n",
    "    def __init__(self, env, gamma, alpha=None, N0=1):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.N0    = N0\n",
    "\n",
    "    \n",
    "    def train(self, policy, max_episodes=1000):\n",
    "        history = {\n",
    "            \"cumulative reward\"  : [],\n",
    "            \"episode error\"      : [],\n",
    "            \"episode length\"     : [],\n",
    "            \"episode win\"        : []\n",
    "        }\n",
    "\n",
    "        Ntable = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "\n",
    "        for _ in tqdm(range(max_episodes)):\n",
    "            state, done = self.env.reset(random_init=True)\n",
    "            cumulative_reward = 0\n",
    "            cumulative_error  = 0\n",
    "\n",
    "            n_steps = 0\n",
    "\n",
    "            while not done:\n",
    "                epsilon = self.N0 / (self.N0 + sum(Ntable[state].values()))\n",
    "\n",
    "                action = policy.act(state, epsilon)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                Ntable[state][action] += 1\n",
    "                n_steps += 1\n",
    "\n",
    "                cumulative_reward = (self.gamma * cumulative_reward) + reward\n",
    "\n",
    "                _, Qmax = policy.max_Q(next_state)\n",
    "\n",
    "                td_error = reward + self.gamma * Qmax - policy.Q(state, action)\n",
    "\n",
    "                cumulative_error += abs(td_error)\n",
    "\n",
    "                alpha = 1/Ntable[state][action] if self.alpha is None else self.alpha\n",
    "\n",
    "                policy.update_Q(state, action, alpha, td_error)\n",
    "\n",
    "                state  = next_state\n",
    "\n",
    "            history[\"cumulative reward\"].append(cumulative_reward)\n",
    "            history[\"episode error\"].append(cumulative_error)\n",
    "            history[\"episode length\"].append(n_steps)\n",
    "            history[\"episode win\"].append(env.is_win())\n",
    "\n",
    "        return policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = QLearning(env.set_render(None), 0.99)\n",
    "q_policy = EpsilonGreedyQAgent()\n",
    "scoresQ = np.zeros([49,100])\n",
    "\n",
    "for i in range (1,50):\n",
    "    q_learning.N0 = i\n",
    "    _, q_history = q_learning.train(q_policy, 10000)\n",
    "\n",
    "    #print(scores)\n",
    "\n",
    "    for j in range(100):\n",
    "        env.run_policy(q_policy, 0, 0)\n",
    "        scoresQ[i-1][j] =  env.get_score()\n",
    "\n",
    "    q_policy.reset()\n",
    "\n",
    "mediaQ = np.mean(scoresQ, 1)\n",
    "print(mediaQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = QLearning(\n",
    "    env    = env.set_render(None),\n",
    "    gamma  = 0.99,\n",
    "    N0     = 10\n",
    ")\n",
    "q_policy, q_history = q_learning.train(EpsilonGreedyQAgent(), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_render('human')\n",
    "env.run_policy(q_policy, 0, .5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes(q_history, \"cumulative reward\", smooth=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA($\\lambda$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(BaseController):\n",
    "    def __init__(self, env, gamma, lmbda, alpha=None, N0=1):\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.alpha = alpha\n",
    "        self.N0    = N0\n",
    "\n",
    "\n",
    "    def train(self, policy, max_episodes=1000):\n",
    "        history = {\n",
    "            \"cumulative reward\"  : [],\n",
    "            \"episode error\"      : [],\n",
    "            \"episode length\"     : [],\n",
    "            \"episode win\"        : []\n",
    "        }\n",
    "\n",
    "        Ntable = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "\n",
    "        for _ in tqdm(range(max_episodes)):\n",
    "            \n",
    "            Etable  = defaultdict(lambda : {action: 0 for action in Actions.actions})\n",
    "\n",
    "            state, done = self.env.reset(random_init = True)\n",
    "            action = policy.act(state, epsilon=1)\n",
    "\n",
    "            cumulative_reward = 0\n",
    "            cumulative_error  = 0\n",
    "\n",
    "            n_steps = 0\n",
    "\n",
    "            while not done:\n",
    "                Etable[state][action] = Etable[state][action] + 1\n",
    "\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                epsilon = self.N0 / (self.N0 + sum(Ntable[next_state].values()))\n",
    "\n",
    "                next_action = policy.act(next_state, epsilon)\n",
    "\n",
    "                Ntable[state][action] += 1\n",
    "                n_steps += 1\n",
    "\n",
    "                cumulative_reward = (self.gamma * cumulative_reward) + reward\n",
    "\n",
    "                td_error = reward + self.gamma * policy.Q(next_state, next_action) - policy.Q(state, action)\n",
    "\n",
    "                cumulative_error += abs(td_error)\n",
    "\n",
    "                alpha = 1/Ntable[state][action] if self.alpha is None else self.alpha\n",
    "\n",
    "                for state in Etable:\n",
    "                    for action in Actions.actions:\n",
    "                        policy.update_Q(state, action, alpha, td_error * Etable[state][action])\n",
    "                        Etable[state][action] *= self.gamma * self.lmbda\n",
    "\n",
    "                state  = next_state\n",
    "                action = next_action\n",
    "\n",
    "            history[\"cumulative reward\"].append(cumulative_reward)\n",
    "            history[\"episode error\"].append(cumulative_error)\n",
    "            history[\"episode length\"].append(n_steps)\n",
    "            history[\"episode win\"].append(env.is_win())\n",
    "        \n",
    "        return policy, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSA(env.set_render(None), 0.99,0.2)\n",
    "sarsa_policy = EpsilonGreedyQAgent()\n",
    "scoresSarsa = np.zeros([49,100])\n",
    "\n",
    "for i in range (1,50):\n",
    "    sarsa.N0 = i\n",
    "    _, sarsa_history = sarsa.train(sarsa_policy, 10000)\n",
    "\n",
    "    #print(scores)\n",
    "\n",
    "    for j in range(100):\n",
    "        env.run_policy(sarsa_policy, 0, 0)\n",
    "        scoresSarsa[i-1][j] =  env.get_score()\n",
    "\n",
    "    sarsa_policy.reset()\n",
    "\n",
    "mediaSarsa = np.mean(scoresSarsa, 1)\n",
    "print(mediaSarsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa = SARSA(\n",
    "    env    = env.set_render(None),\n",
    "    gamma  = 0.99,\n",
    "    lmbda  = 0.2,\n",
    "    N0     = 10\n",
    ")\n",
    "sarsa_policy, sarsa_history = sarsa.train(EpsilonGreedyQAgent(), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_render('human')\n",
    "env.run_policy(sarsa_policy, 0, .5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_episodes(sarsa_history, \"episode win\", smooth=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyLinearAgent(Agent):\n",
    "    \"\"\"\n",
    "    Agent with linear approximation for Q and epsilon-greedy policy.\n",
    "    This class do not include any default training algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.weights = np.random.randn(action_dim, state_dim)\n",
    "    \n",
    "    \n",
    "    def act(self, state, epsilon=0.) -> str:\n",
    "        if np.random.rand() > epsilon:\n",
    "            action, _ = self.max_Q(state)\n",
    "\n",
    "        else:\n",
    "            action = Actions.sample()\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def Q(self, state, action):\n",
    "        idx = Actions.action_index(action)\n",
    "\n",
    "        return (self.weights @ np.array(state))[idx]\n",
    "\n",
    "\n",
    "    def max_Q(self, state):\n",
    "        Qvalues = self.weights @ np.array(state)\n",
    "\n",
    "        idx = np.argmax(Qvalues)\n",
    "\n",
    "        return Actions.actions[idx], Qvalues[idx]\n",
    "\n",
    "\n",
    "    def update_Q(self, state, action, alpha, error):\n",
    "        bellman_loss_grad = np.array(state)\n",
    "\n",
    "        self.weights += alpha * error * bellman_loss_grad\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset agent's Q table.\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(*self.weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_env = PacmanEnv.contourDanger(4, ghost_names=\"RobustGhost\", state_space=\"features\")\n",
    "\n",
    "monte_carlo = MonteCarloControl(\n",
    "    env    = linear_env.set_render(None),\n",
    "    gamma  = 0.99,\n",
    "    alpha  = 0.1,\n",
    ")\n",
    "mc_policy, mc_history = monte_carlo.train(EpsilonGreedyLinearAgent(2, 4), 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning = QLearning(\n",
    "    env    = linear_env.set_render(None),\n",
    "    gamma  = 0.99,\n",
    "    N0     = 10\n",
    ")\n",
    "q_policy, q_history = q_learning.train(EpsilonGreedyLinearAgent(2, 4), 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
